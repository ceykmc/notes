---
title: 梯度裁剪及其作用
tags: 机器学习, 深度学习
notebook: 机器学习
---

梯度裁剪一般用于解决梯度爆炸(gradient explosion)问题，而梯度爆炸问题在训练RNN过程中出现得尤为频繁，所以训练RNN基本都需要带上这个参数。常见的gradient clipping有两种做法：

1. 根据参数的gradient的值直接进行裁剪。
2. 根据若干参数的gradient组成的vector的L2 norm进行裁剪。

第一种做法很容易理解，就是先设定一个gradient的范围。譬如设定范围为 (-1, 1), 小于-1的gradient设为 -1，大于1的 gradient设为 1。

第二种方法则更为常见，先设定一个`clip_norm`，每次反向传播计算得到各个参数的gradient后，将各个参数的 gradient 构成一个 vector，计算这个 vector 的 L2 norm，记为 `LNorm`。比较 `LNorm` 和 `clip_norm` 的值，若 `LNorm` <= `clip_norm` 不做处理，否则计算缩放因子 `scale_factor` = `clip_norm`/`LNorm` ，然后令原来的梯度乘上这个缩放因子。**这样做是为了让 gradient vector 的 L2 norm 小于预设的 `clip_norm`**。

![梯度裁剪](http://image.liyumu.cn/gradient_clip.png)

---

参考：

[梯度裁剪及其作用](https://wulc.me/2018/05/01/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8/)

[What is gradient clipping and why is it necessary?](https://www.quora.com/What-is-gradient-clipping-and-why-is-it-necessary)